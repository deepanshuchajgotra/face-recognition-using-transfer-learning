{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Flower Classifier with VGG16\n",
    "\n",
    "### Loading the VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "# VGG16 was designed to work on 64 x 64 pixel input images sizes\n",
    "img_rows = 64\n",
    "img_cols = 64 \n",
    "\n",
    "#Loads the VGG16 model \n",
    "model = VGG16(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpsecting each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D True\n",
      "2 Conv2D True\n",
      "3 MaxPooling2D True\n",
      "4 Conv2D True\n",
      "5 Conv2D True\n",
      "6 MaxPooling2D True\n",
      "7 Conv2D True\n",
      "8 Conv2D True\n",
      "9 Conv2D True\n",
      "10 MaxPooling2D True\n",
      "11 Conv2D True\n",
      "12 Conv2D True\n",
      "13 Conv2D True\n",
      "14 MaxPooling2D True\n",
      "15 Conv2D True\n",
      "16 Conv2D True\n",
      "17 Conv2D True\n",
      "18 MaxPooling2D True\n"
     ]
    }
   ],
   "source": [
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's freeze all layers except the top 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D False\n",
      "2 Conv2D False\n",
      "3 MaxPooling2D False\n",
      "4 Conv2D False\n",
      "5 Conv2D False\n",
      "6 MaxPooling2D False\n",
      "7 Conv2D False\n",
      "8 Conv2D False\n",
      "9 Conv2D False\n",
      "10 MaxPooling2D False\n",
      "11 Conv2D False\n",
      "12 Conv2D False\n",
      "13 Conv2D False\n",
      "14 MaxPooling2D False\n",
      "15 Conv2D False\n",
      "16 Conv2D False\n",
      "17 Conv2D False\n",
      "18 MaxPooling2D False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here we freeze the last 4 layers \n",
    "# Layers are set to trainable as True by default\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a function that returns our Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTopModel(bottom_model, num_classes, D=256):\n",
    "    \"\"\"creates the top or head of the model that will be \n",
    "    placed ontop of the bottom layers\"\"\"\n",
    "    top_model = bottom_model.output\n",
    "    top_model = Flatten(name = \"flatten\")(top_model)\n",
    "    top_model = Dense(D, activation = \"relu\")(top_model)\n",
    "    top_model = Dropout(0.3)(top_model)\n",
    "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add our FC Head back onto VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 15,240,517\n",
      "Trainable params: 525,829\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "FC_Head = addTopModel(model, num_classes)\n",
    "\n",
    "modelnew = Model(inputs=model.input, outputs=FC_Head)\n",
    "\n",
    "print(modelnew.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Flowers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 122 images belonging to 5 classes.\n",
      "Found 15 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = 'Dataset/Train/'\n",
    "validation_data_dir = 'Dataset/Test/'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 16\n",
    "val_batchsize = 10\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our top layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "12/12 [==============================] - 6s 538ms/step - loss: 1.5856 - accuracy: 0.3602 - val_loss: 1.2094 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.20944, saving model to Celeb_VSS16.h5\n",
      "Epoch 2/15\n",
      "12/12 [==============================] - 6s 493ms/step - loss: 1.3375 - accuracy: 0.4722 - val_loss: 1.8214 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.20944\n",
      "Epoch 3/15\n",
      "12/12 [==============================] - 6s 522ms/step - loss: 1.1930 - accuracy: 0.5167 - val_loss: 0.8026 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.20944 to 0.80263, saving model to Celeb_VSS16.h5\n",
      "Epoch 4/15\n",
      "12/12 [==============================] - 7s 574ms/step - loss: 1.0952 - accuracy: 0.5538 - val_loss: 0.5236 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.80263 to 0.52360, saving model to Celeb_VSS16.h5\n",
      "Epoch 5/15\n",
      "12/12 [==============================] - 6s 499ms/step - loss: 0.9716 - accuracy: 0.6290 - val_loss: 0.5225 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.52360 to 0.52247, saving model to Celeb_VSS16.h5\n",
      "Epoch 6/15\n",
      "12/12 [==============================] - 8s 689ms/step - loss: 1.0848 - accuracy: 0.5833 - val_loss: 1.2429 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.52247\n",
      "Epoch 7/15\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.9261 - accuracy: 0.5944 - val_loss: 0.3575 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.52247 to 0.35754, saving model to Celeb_VSS16.h5\n",
      "Epoch 8/15\n",
      "12/12 [==============================] - 8s 686ms/step - loss: 0.8164 - accuracy: 0.6882 - val_loss: 0.7523 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35754\n",
      "Epoch 9/15\n",
      "12/12 [==============================] - 8s 691ms/step - loss: 0.7592 - accuracy: 0.7097 - val_loss: 0.3253 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35754 to 0.32531, saving model to Celeb_VSS16.h5\n",
      "Epoch 10/15\n",
      "12/12 [==============================] - 8s 667ms/step - loss: 0.8746 - accuracy: 0.6611 - val_loss: 0.7019 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.32531\n",
      "Epoch 11/15\n",
      "12/12 [==============================] - 8s 673ms/step - loss: 0.7948 - accuracy: 0.6720 - val_loss: 0.3939 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.32531\n",
      "Epoch 12/15\n",
      "12/12 [==============================] - 8s 699ms/step - loss: 0.6314 - accuracy: 0.7833 - val_loss: 0.2775 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.32531 to 0.27752, saving model to Celeb_VSS16.h5\n",
      "Epoch 13/15\n",
      "12/12 [==============================] - 6s 490ms/step - loss: 0.6447 - accuracy: 0.7742 - val_loss: 0.3434 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.27752\n",
      "Epoch 14/15\n",
      "12/12 [==============================] - 6s 473ms/step - loss: 0.6911 - accuracy: 0.7000 - val_loss: 0.2178 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.27752 to 0.21780, saving model to Celeb_VSS16.h5\n",
      "Epoch 15/15\n",
      "12/12 [==============================] - 6s 486ms/step - loss: 0.6000 - accuracy: 0.7500 - val_loss: 0.4354 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.21780\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "                   \n",
    "checkpoint = ModelCheckpoint(\"Celeb_VSS16.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint]\n",
    "\n",
    "# Note we use a very small learning rate \n",
    "modelnew.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 122\n",
    "nb_validation_samples = 15\n",
    "epochs = 15\n",
    "batch_size = 10\n",
    "\n",
    "history = modelnew.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)\n",
    "\n",
    "modelnew.save(\"Celeb_VSS16.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
